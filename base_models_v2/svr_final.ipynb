{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Additional performance metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_combined_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df['week'] = pd.to_numeric(df['week'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# Add date_ordinal\n",
    "df['date_ordinal'] = df['date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Add cyclic month representation\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Add cyclic week representation\n",
    "df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
    "\n",
    "df[\"year_sin\"] = np.sin(2 * np.pi * df[\"year\"] / df[\"year\"].max())\n",
    "df[\"year_cos\"] = np.cos(2 * np.pi * df[\"year\"] / df[\"year\"].max())\n",
    "\n",
    "# df[\"cases_rolling_mean\"] = df.groupby(\"geocode\")[\"cases\"].transform(lambda x: x.rolling(4).mean())\n",
    "\n",
    "\n",
    "# Step 2: Create lag features\n",
    "def create_lags(dataframe, group_col, target_col, lags, inplace = False):\n",
    "    if isinstance(target_col, list):  # If target_col is a list of columns\n",
    "        for col in target_col:\n",
    "            for lag in lags:\n",
    "                if inplace:\n",
    "                    dataframe[target_col] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "                else:\n",
    "                    dataframe[f'{col}_lag{lag}'] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "    else:  # If target_col is a single column\n",
    "        for lag in lags:\n",
    "            if inplace:\n",
    "                dataframe[target_col] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "            else:\n",
    "                dataframe[f'{target_col}_lag{lag}'] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "    return dataframe\n",
    "\n",
    "# Lag cases by 1 and 2 weeks\n",
    "data = create_lags(df, group_col='city', target_col='cases', lags=[0,1,-2])\n",
    "\n",
    "# Lag weather-related variables by 5 and 6 weeks for each city\n",
    "weather_columns = ['tempe_min', 'temp_avg', 'temp_max', 'humidity_max', 'humidity_avg', 'humidity_min',\n",
    "                   'precipitation_avg_ordinary_kriging', 'precipitation_max_ordinary_kriging',\n",
    "                   'precipitation_avg_regression_kriging', 'precipitation_max_regression_kriging']\n",
    "data = create_lags(df, group_col='city', target_col=weather_columns, lags=[4, 5, 6])\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "    ('onehot', OneHotEncoder(sparse_output=False), ['geocode'])  # One-hot encode 'geocode'\n",
    "], remainder='passthrough')  # Keep all other columns as is\n",
    "\n",
    "transformed_data = ct.fit_transform(data)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = ct.get_feature_names_out()\n",
    "\n",
    "# Clean feature names to remove 'remainder__' prefix\n",
    "cleaned_feature_names = [name.split('__')[-1] if 'remainder__' in name else name for name in feature_names]\n",
    "\n",
    "# Convert transformed data back to DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=cleaned_feature_names)\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64','int32']).columns\n",
    "for col in numeric_cols:\n",
    "    if col in transformed_df.columns:\n",
    "        transformed_df[col] = pd.to_numeric(transformed_df[col], errors='coerce')\n",
    "\n",
    "if 'date' in transformed_df.columns:\n",
    "    transformed_df['date'] = pd.to_datetime(transformed_df['date'], errors='coerce')\n",
    "\n",
    "print(transformed_df.dtypes)\n",
    "data= transformed_df\n",
    "\n",
    "# Now 'transformed_df' contains the transformed data with the correct types\n",
    "print(transformed_df.dtypes) \n",
    "\n",
    "train_data = data[data['date'].dt.year <= 2020]\n",
    "test_data = data[data['date'].dt.year >= 2021]\n",
    "\n",
    "X_train = train_data.drop(columns=['cases','cases_per_100k','cases_lag-2'])\n",
    "y_train = train_data[['cases_lag-2']]\n",
    "\n",
    "X_test = test_data.drop(columns=['cases','cases_per_100k','cases_lag-2'])\n",
    "y_test = test_data[['cases_lag-2']]\n",
    "\n",
    "data = data.drop(columns=['cases_per_100k'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Step 2: Define which features you'll actually use for modeling\n",
    "feature_cols = [\n",
    "   'cases_lag0', 'cases_lag1',\n",
    "    'temp_avg_lag4', 'humidity_avg_lag4',\n",
    "    'precipitation_max_regression_kriging_lag4',\n",
    "    'week_sin', 'month_sin', 'week_cos', 'month_cos',\n",
    "    'year_sin', 'year_cos'\n",
    "]\n",
    "\n",
    "# Add one-hot encoded columns\n",
    "selected_columns = feature_cols + [col for col in data.columns if col.startswith('onehot_')]\n",
    "\n",
    "# Step 3: Prepare feature matrix with only selected columns\n",
    "X_train = X_train[selected_columns]\n",
    "X_test = X_test[selected_columns]\n",
    "\n",
    "# Step 4: Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Step 5: Scale features - only scale numeric features within selected_columns\n",
    "numeric_features = [col for col in X_train.columns if X_train[col].dtype in ['float64', 'int64', 'float32', 'int32'] \n",
    "                   and not col.startswith('onehot_')]\n",
    "\n",
    "X_train[numeric_features] = feature_scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = feature_scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Step 6: Scale target variable (ensuring proper shape)\n",
    "y_train_reshaped = y_train.values.reshape(-1, 1)\n",
    "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train_reshaped).ravel()\n",
    "y_test_scaled = target_scaler.transform(y_test_reshaped).ravel()\n",
    "\n",
    "# Store best model and its performance\n",
    "# Parameter combinations\n",
    "C_values = [0.1]\n",
    "epsilon_values = [0.0001]\n",
    "kernel = \"linear\"  # Only using linear kernel\n",
    "\n",
    "# Initialize best RMSE tracker\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "# Iterate through parameter combinations\n",
    "for C in C_values:\n",
    "    for epsilon in epsilon_values:\n",
    "        # Create SVR model with current parameters\n",
    "        svr_params = {'C': C, 'epsilon': epsilon, 'kernel': kernel}\n",
    "        svr = SVR(**svr_params)\n",
    "        svr.fit(X_train, y_train_scaled)\n",
    "\n",
    "        # Predict and calculate RMSE\n",
    "        y_pred_scaled = svr.predict(X_test)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "        y_test_original = target_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Calculate Root Mean Squared Error\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Epsilon: {epsilon}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # Update best model if current model performs better\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = svr\n",
    "            best_params = svr_params\n",
    "\n",
    "print(\"\\nBest Model Parameters:\", best_params)\n",
    "print(\"Best RMSE:\", best_rmse)\n",
    "\n",
    "\n",
    "# Final predictions using best model\n",
    "y_pred_scaled = best_model.predict(X_test)\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "y_test_original = target_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "# Final predictions using best model for the training set\n",
    "y_train_pred_scaled = best_model.predict(X_train)\n",
    "y_train_pred = target_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_train_pred = np.clip(y_train_pred, 0, None)\n",
    "\n",
    "y_train_original = target_scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate R-squared (R²) for the train set\n",
    "r2_train = r2_score(y_train_original, y_train_pred)\n",
    "print(f\"R-squared (R²) for the train set: {r2_train:.4f}\")\n",
    "\n",
    "# Prepare test dataframe with results\n",
    "X_test_df = pd.DataFrame(X_test, columns=selected_columns)\n",
    "X_test_df[\"actual\"] = y_test_original\n",
    "X_test_df[\"predicted\"] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(f\"R-squared (R²) for the test set: {r2:.4f}\")\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test_original, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape * 100, \"%\")\n",
    "\n",
    "# Plot Actual vs Predicted for each one-hot encoded geocode column\n",
    "for geocode in X_test_df.columns:\n",
    "    if geocode.startswith('onehot__geocode_'):\n",
    "        # Filter the subset for rows where the current geocode column is 1\n",
    "        subset = X_test_df[X_test_df[geocode] == 1]\n",
    "        \n",
    "        # Plot actual vs predicted cases\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(subset.index, subset[\"actual\"], label='Actual')\n",
    "        plt.plot(subset.index, subset[\"predicted\"], label='Predicted')\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Cases\")\n",
    "        plt.title(f\"Actual vs Predicted for Geocode {geocode}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
